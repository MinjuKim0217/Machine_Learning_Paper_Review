{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT모델 구현.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL8QUiZl-Gb-",
        "outputId": "7b7bfbb8-af0b-4348-d973-a26a7edd4410"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 6.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=7ec56e713a582716e53fc3a8eef98bdb7d44ae482586f660a5f52092207484fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0_O6rzz-H-M",
        "outputId": "aa25da94-8bac-4d15-a323-29aaeeb50492"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# data를 저장할 폴더 입니다. 환경에 맞게 수정 하세요.\n",
        "data_dir = \"/content/drive/My Drive/BERT_DATA\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZqb7Ac3-Q8p"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from random import random, randrange, randint, shuffle, choice\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm, tqdm_notebook, trange\n",
        "import sentencepiece as spm\n",
        "import wget\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGDcDs8a-Y0H",
        "outputId": "15d09109-cb5d-40d4-e5ea-bbe5e5edd599"
      },
      "source": [
        "for f in os.listdir(data_dir):\n",
        "  print(f)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kowiki.vocab\n",
            "kowiki_t5.vocab\n",
            "ratings_test.txt\n",
            "kowiki.model\n",
            "kowiki_t5.model\n",
            "ratings_test.json\n",
            "ratings_test_t5.json\n",
            "ratings_train_t5.json\n",
            "ratings_train.txt\n",
            "ratings_train.json\n",
            "save_gpt_pretrain.pth\n",
            "save_t5_pretrain.pth\n",
            "save_bert_pretrain.pth\n",
            "kowiki.csv.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISzKcKdY-aT7",
        "outputId": "c39ba108-1b50-4f97-a39b-4fc1062ce5a4"
      },
      "source": [
        "# vocab loading\n",
        "vocab_file = f\"{data_dir}/kowiki.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8qLnEVw-10X"
      },
      "source": [
        "### config 설정하기\n",
        "\n",
        "BERT는 Encoder만 사용하므로 항목 중 Decoder 부분은 제거 했습니다.\n",
        "BERT Encoder는 기본 입력에 추가로 Segment 정보를 입력 받는데 Segment개수를 정의하는 n_seg_type을 추가로 정의 했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhPT8HAI-tEK"
      },
      "source": [
        "\"\"\" configuration json을 읽어들이는 class \"\"\"\n",
        "class Config(dict): \n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHM0J5kt-7bI",
        "outputId": "b0e8dced-91a9-4cbe-a9e8-56525d9bed37"
      },
      "source": [
        "config = Config({\n",
        "    \"n_enc_vocab\": len(vocab),\n",
        "    \"n_enc_seq\": 256,\n",
        "    \"n_seg_type\": 2,\n",
        "    \"n_layer\": 6,\n",
        "    \"d_hidn\": 256,\n",
        "    \"i_pad\": 0,\n",
        "    \"d_ff\": 1024,\n",
        "    \"n_head\": 4,\n",
        "    \"d_head\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"layer_norm_epsilon\": 1e-12\n",
        "})\n",
        "print(config)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_enc_vocab': 8007, 'n_enc_seq': 256, 'n_seg_type': 2, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDMPdb8j_G3P"
      },
      "source": [
        "### Common Class\n",
        "\n",
        "공통으로 사용되는 Class 및 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_Yth38I-9Qh"
      },
      "source": [
        "# sigusoid position encoding\n",
        "\n",
        "def get_sigusoid_encoding_table(n_seq, d_hidn):\n",
        "  def cal_angle(position, i_hidn):\n",
        "    return position/np.power(10000, 2*(i_hidn//2)/d_hidn)\n",
        "  def get_posi_angle_vec(position):\n",
        "    return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
        "\n",
        "  sinusoid_table=np.array([get_posi_angle_vec(i_seq) for i_seq in range (n_seq)])\n",
        "  sinusoid_table[:,0::2]=np.sin(sinusoid_table[:,0::2]) #even index sin\n",
        "  sinusoid_table[:,1::2]=np.cos(sinusoid_table[:,1::2]) #odd index cas\n",
        "\n",
        "  return sinusoid_table\n",
        "\n",
        "  \"\"\" attention pad mask \"\"\"\n",
        "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # <pad>\n",
        "    return pad_attn_mask\n",
        "\n",
        "\n",
        "\"\"\" attention decoder mask \"\"\"\n",
        "def get_attn_decoder_mask(seq):\n",
        "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
        "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
        "    return subsequent_mask\n",
        "\n",
        "# scale dot product attention \n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
        "        attn_prob = self.dropout(attn_prob)\n",
        "        # (bs, n_head, n_q_seq, d_v)\n",
        "        context = torch.matmul(attn_prob, V)\n",
        "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
        "        return context, attn_prob\n",
        "\"\"\" multi head attention \"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
        "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    \n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        batch_size = Q.size(0)\n",
        "        # (bs, n_head, n_q_seq, d_head)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "        # (bs, n_head, n_k_seq, d_head)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "        # (bs, n_head, n_v_seq, d_head)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
        "\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
        "    # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
        "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
        "        # (bs, n_head, n_q_seq, e_embd)\n",
        "        output = self.linear(context)\n",
        "        output = self.dropout(output)\n",
        "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
        "        return output, attn_prob\n",
        "\n",
        "\n",
        "\"\"\" feed forward \"\"\"\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
        "        self.active = F.gelu\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # (bs, d_ff, n_seq)\n",
        "        output = self.active(self.conv1(inputs.transpose(1, 2)))\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        # (bs, n_seq, d_hidn)\n",
        "        return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ2_dzXCBkPd"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "표준 transformer Encoder 에서 BERT에서 추가된 정의한 segment embedding만 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ziZBM_TAMQ6"
      },
      "source": [
        "# 표준 Transformer EncoderLayer와 동일하다\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config=config\n",
        "\n",
        "    self.self_attn=MultiHeadAttention(self.config)\n",
        "    self.layer_norm1=nn.LayerNorm(self.config_hidn, eps=self.config.layer_norm_epsilon)\n",
        "    self.pos_ffn=PoswiseFeedForwardNet(self.config)\n",
        "    self.layer_norm2=nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
        "\n",
        "\n",
        "  def forward(self, inputs, attn_mask):\n",
        "    attn_outputs, attn_prob=self.self_attn(inputs, inputs, inputs, attn_mask)\n",
        "    attn_outputs=self.layer_norm1(inputs+attn_outputs)\n",
        "    ffn_outputs=self.pos_ffn(attn_outputs)\n",
        "    ffn_outputs=self.layer_norm2(ffn_outputs+attn_outputs)\n",
        "\n",
        "    return ffn_outputs, attn_prob"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhvHILQQCt2r"
      },
      "source": [
        "## encoder\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config=config\n",
        "    self.enc_emb=nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
        "    self.pos_emb=nn.Embedding(self.config.n_enc_seq+1, self.config.d_hidn)\n",
        "    self.seg_emb=nn.Embedding(self.config.n_seg_type, self.config.d_hidn)\n",
        "    self.layers=nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXv4aN66HS0z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}